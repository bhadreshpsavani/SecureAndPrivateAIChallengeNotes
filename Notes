Day1: 16-06-2019

----------------Gradient Descent-------------

By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and 
the network is able to predict the correct labels with high accuracy. 

We find this minimum using a process called gradient descent.

------------Why BackPropogation---------------

For single layer networks, gradient descent is straightforward to implement. 

Training multilayer networks is done through backpropagation which is really just an application of the chain rule from calculus.

Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.
We update our weights using this gradient with some learning rate  ùõº .

-------------------Autograde-------------------------
torch provides a module, autograd, for automatically calculating the gradients of tensors.

-------------steps to build model using pytorch------------------
Building step:
1. Build Model
2. define Loss and Optimizer

Training step:
4. Transform and Load:
    1. Transform
    2. Download and Load Training Data
    3. Download and Load Testing Data

5. Passdata to model (Forward Pass to get logits) 
6. Calculate losses
7. Updates hyperparameters using Gradient Decent

Testing step:
8. Test model and  get Accuracy

----------------Why model.eval()?-------------------

During traning we want to use droupout layer to prevent overfitting, but during inference(Testing, Evaluation) we don't use droupout layers
for that we need to use 
model.eval() #to set model in evaluation mode
model.train() #to set model in training mode

Day2: 17-06-2019

The parameters for PyTorch networks are stored in a model's state_dict, we can access it using "model.state_dict()"

Save model para: torch.save(model.state_dict(), 'checkpoint.pth')
Load model para: state_dict = torch.load('checkpoint.pth') #get checkpoint
                 model.load_state_dict(state_dict) #get trained model

Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture.

This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved 
in the checkpoint, along with the state dict. To do this,you build a dictionary with all the information you need to compeletely 
rebuild the model.
