Day1: 16-06-2019

----------------Gradient Descent-------------

By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and 
the network is able to predict the correct labels with high accuracy. 

gradient is always points in the direction of fastest change

We find this minimum using a process called gradient descent.

------------Why BackPropogation---------------

For single layer networks, gradient descent is straightforward to implement. 

Training multilayer networks is done through backpropagation which is really just an application of the chain rule from calculus.

Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.
We update our weights using this gradient with some learning rate  𝛼 .

-------------------Autograde-------------------------
torch provides a module, autograd, for automatically calculating the gradients of tensors.

If we have tensor we want to calculate gradient for it then Autograd will automatically calculate gradient for us,

we have have top tell that x=tensor.zeros(require_grad=true) to keep track of all the changes so when time comes we can do back 
propogation to calculate gradient.

we can turn it off using "with tensor.no_grade():"

set gradient on and off globally using "torch.set_grad_enabled(True|False)"

Pytorch keeps accumulate gradients, so we need to clear gradient in every training backward passes using "optimizer.zero_grad()"

-------------steps to build model using pytorch------------------
Building step:
1. Build Model
2. define Loss and Optimizer

Training step:
4. Transform and Load:
    1. Transform
    2. Download and Load Training Data
    3. Download and Load Testing Data

5. Passdata to model (Forward Pass to get logits) 
6. Calculate losses
7. Updates hyperparameters using Gradient Decent

Testing step:
8. Test model and  get Accuracy

----------------Why model.eval()?-------------------

During traning we want to use droupout layer to prevent overfitting, but during inference(Testing, Evaluation) we don't use droupout layers
for that we need to use 
model.eval() #to set model in evaluation mode
model.train() #to set model in training mode

Day2: 17-06-2019

The parameters for PyTorch networks are stored in a model's state_dict, we can access it using "model.state_dict()"

Save model para: torch.save(model.state_dict(), 'checkpoint.pth')
Load model para: state_dict = torch.load('checkpoint.pth') #get checkpoint
                 model.load_state_dict(state_dict) #get trained model

Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture.

This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved 
in the checkpoint, along with the state dict. To do this,you build a dictionary with all the information you need to compeletely 
rebuild the model.

Day3 : 19-06-2019
How broadcasting works in pytorch?

for computational reasons if we are nn.CrossEntropyLoss() then we have to pass logits not the probabilities in the function.

Adam vs SGD? Adam has momentum which speeds up the actual fitting process and also adjust learning rate for each of the individual porameters in your model

What should be the matrix is up to the developer what he want to optimize, a lot of time it is accuracy, It can also be top 5 error rate, 
precision recall

nn.NLLLoss(): Negative log liklihood loss

Quick question:> Transforms increases data or not?

-----------------------------------Secure and Private AI----------------------------------

Federate Learning
Differential Privacy: when model learn from sensative data, it should learn what it suppose to learn
